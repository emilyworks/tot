## Overview
This project is an exploratory extension of [Tree-of-Thought (Yao et al., 2023)](https://github.com/princeton-nlp/tree-of-thought-llm) (referred to as ToT in this repository).
You can find the GPT ToT implementation with RAG and tools in [this repo](https://github.com/r1p71d3/tot-math).

We were specifically driven to extend upon ToT by the following:

**Problem Motivation**: Complex math problems can be difficult for LLMs to solve reliably. At the same time, ToT has shown promise for helping LLMs tackle complex reasoning problems.

**Problem Goal**: Explore the efficacy and efficiency of solving complex math problems with Tree-of-Thought.

(**NOTE:** We originally intended to adapt the ToT source code repository for this project but due to a lack of fit, we eventually deforked and wrote our own repository implementations from scratch instead. However, please note that the deforking process has still preserved the commit history and Github contributor list from when the repository was forked. Please look to recent commits to see the code and contributors relevant to this specific project, as we did not end up using code from the original ToT repository.)

The rest of the README is structured as follows:
- Quickstart
- Args (full descriptions of each optional argument that can be used during runs)
- Repository/Directory Structure
- Results and Brief Observations (for a full discussion, please see our final report)

## Quickstart
Ensure you are in the main branch for Llama runs, and the 'gpt' branch for GPT-4o runs. If necessary, run ```pip install -r requirements.txt``` from the root directory to install any missing required packages.

As the GPT runs require an OpenAI API key, please make sure to export it as an OPENAI_API_KEY env variable. You can also place a `.env` file containing the variable in the project directory. Please note that ToT req

Then, from the root directory, run ```python run_bench.py``` followed by your desired parameters.

A list of all available args can be found below.

## Args
Please note all args are optional. There are several args used in this project that control the nature of each ToT run. For readability, they are grouped by purpose as follows:

### Type of agent
- ```agent``` supports three options: 
    - "zeroshot" for the basic zero-shot prompt
    - "cot" for the Chain of Thought structure
    - "tot" for the Tree of Thought structure
      
(**NOTE:** Our attempt at implementing ToT with RAG and tools can be found [here](https://github.com/r1p71d3/tot-math).)

### Model generation configurations
- ```temperature``` modifies the token probabilities. Default is 0.7 to encourage meaningfully diverse 'branches' from each parent node in the tree.

### Tree structure and traversal
- ```a_star``` . If this flag is present, the code will run ToT using the A* traversal method.
- ```queue_size``` controls the maximum size of the priority queue used to help implement the A* traversal method in this repository. The top q-size ranked proposals are selected after each proposal-evaluation iteration and carried forward into the next. Default is 7. 
- ```depth``` controls the "depth" of the ToT tree and, effectively, the maximum number of attempts a model is allowed to reach its final solution. Default is 15.
- ```breadth``` controls the branching factor of each node in the tree and, effectively, the number of proposals generated by the model per call. Default is 3.


## Repository Structure
This branch contains all code necessary to run GPT with the original implementation of ToT on the processed MATH dataset. The tool-augmented implementation can be found [here](https://github.com/r1p71d3/tot-math). Please switch to the `main` branch for the Llama implementation.

This repository is generally structured as follows:

```run_bench.py``` is the main driver script and will run GPT for the project.

```inputs.csv``` contains the 50 samples from the MATH evaluation dataset

```src/tot/``` holds the scripts for the main inference runs. It holds the following:
- ```prompts/bench.py``` holds the model prompts used for the proposal and evaluation stages during the ToT run

## Results
We obtained the following accuracy scores:
- **Zero-Shot**: 94%
- **CoT**: 100%
- **ToT**: 70%
- **ToT + RAG and tools**: 84%

Please refer to the report for the details and interpretation of the results.
